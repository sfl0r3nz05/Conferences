\section{NLP Engine methodology}
Overall, the logic designed for the NLP engine follows two approaches: \textit{detection} and \textit{comparison}. While \textit{detection} has symbols as basic processing unit, \textit{comparison} has sub-articles as basic processing unit. \textit{Detection} represents the capability to detect \textit{variables} in a text file, i.e., the Roaming Agreement. In this regard, \textit{Amazon Comprehend} constitutes the enabling technology of the \textit{detection} approach, since it is a service that uses NLP techniques to extract insights about the content document by recognizing  the  entities,  key  phrases,  language,  sentiments,  and  other  common  elements  in  a  text \cite{AWS2021}.

\textit{Comparison} represents the capability to find \textit{similarities} and differences between the sub-articles present in the Roaming Agreement with respect to the sub-articles present in the GSMA standard template. In this regard, \textit{similarity} constitutes the enabling technology of the \textit{comparison} approach, since it is a resource commonly used for pattern classification, clustering and information retrieval problems \cite{7429408}. In this regard, our work uses  \textit{Jaccard's similarity} which is defined as the size of the intersection divided by the size of the union of two sets \cite{Gupta2018}. As result of the \textit{comparison}, it is determined that while an almost total coincidence between texts at the sub-article level represents a \textit{standard clause}, an almost null coincidence between texts (or simply the non-existence of a sub-article of the GSMA standard template in the Roaming Agreement) represents a \textit{customized text}. Thus, the intermediate case is represented by the \textit{variation} in which there is a high coincidence between sub-articles and the existing differences are given by the presence of \textit{variables} such as the commercial names of MNOs and the start date of the RA. The next section integrates tools, NLP techniques, pre-processing and post-processing as part of the designed methodology.

\subsection{Designed Methodology}
The flow chart in Fig.~\ref{fig1} is a general scheme of the methodology designed for the NLP Engine, which will be explained in detail below.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.4\textwidth]{images/methodology.png}}
\caption{Overview of the designed methodology.}
\label{fig1}
\end{figure}

The starting point of the methodology is the detection of PDF files. The file name is used as one of the identifiers of the output file. The next step is to find similarities in document texts so that headers and footers are detected to avoid undesired characters. For that purpose, the mechanism used is the detection of different font size and weight. Once the PDF format has been converted to text, headers and footers as well as undesired characters are removed. It is now possible to perform both \textit{detection} and \textit{comparison}.

The \textit{detection} phase includes a requirement associated with the Amazon Comprehend tool regarding the number of symbols to be sent via REST API. For this reason, the text is segmented into pieces of 100 symbols. For each symbol packet the API outputs entities, key phrases and sentiment in a JSON format (\cite{AWS2021}), which must be grouped and further processed. The designed logic collects the most important fields such as: 'BeginOffset', 'EndOffset', 'Score', 'Text', 'Type', 'PartOfSpeech' and 'Tag'. In turn, it adds to these the 'Frequency' field that stores the number of repetitions of the entity stored in the 'Text' field. Once this filtering has been done, entities are sorted according to a logic that defines their relevance degree. The designed logic is particularized according to the type of \textit{variable} to be detected. For instance, for the name of the MNO, the 'Score', 'Frequency' and 'BeginOffset' are prioritized, in addition to verifying the number of occurrences within the key phrases. The detected \textit{variables} are stored to be used later to determine which ones are part of the \textit{variations} or \textit{customized texts}.

The \textit{comparison} phase implies to separate by articles and then by sub-articles. Since, several of possibilities of articles names are loading in the NLP Engine is easy to locate the corresponding article in the GSMA standard template and therefore, to establish comparisons at sub-article level. \textit{Jaccard's similarity} associates a score to the sub-article id. Thus, an sub-article is considered a \textit{standard clause} whether the score assigned is greater than 0.85 (score $\geq$ 0.85). In addition, a sub-article is considered a \textit{custom text} whether the score assigned is less than 0.15 (score $\leq$ 0.15). Finally, a sub-article is considered a \textit{variation} whether the score assigned is between 0.85 and 0.15 (0.15 $<$ score $<$ 0.85). At this point, the NLP Engine is ready to populate the output file, inspecting in case of to have tagged the sub-articles as \textit{variation} or \textit{customized texts}, the \textit{variables} that it contains.

\section{Implemented System}
The Fig.~\ref{fig2} shows the overall architecture of the NLP engine integrated in a docker infrastructure that includes 3 parts. Thus, the input files include the Roaming Agreement text file as well as the GSMA standard templates. The processing layer includes the logic associated with the NLP engine, i.e. the implementation of the designed methodology. The output file constitutes a JavaScript Object Notation (JSON) file populated with the classification of articles and sub-articles as \textit{standard clauses}, \textit{customized texts} and \textit{variations}. In addition, each article includes the set of \textit{variables} it contains and each sub-article contains the specific \textit{variable} detected as long as it has been classified as \textit{variation} or \textit{customized texts}.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.23\textwidth]{images/NLP_Engine.png}}
\caption{NLP Engine overall architecture.}
\label{fig2}
\end{figure}

The input and output files are associated to the container through volumes. In addition, the NLP Engine has been developed as a Python v3.8 based library and therefore must be imported and run from an entry point. Although the NLP Engine constitutes a library, it must in turn integrate other libraries, among which boto3 \cite{boto3}. Boto3 is an Software Development Kit (SDK) for Python, created and supported by Amazon Web Services (AWS). The NLP Engine imports the specific service associated to Amazon Comprehend. PyMuPDF (\cite{PyMuPDF}) constitutes other library imported into the NLP Engine to be used into headers and footers detection. Therefore, the docker image built for the NLP Engine must not only install the NLP Engine library, but must also install the libraries it imports. The \textit{similarity} has been implemented in a pluggable way, therefore at code level it is easy to change the \textit{Jaccard's similarity} for another one, e.g., cosine similarity \cite{Gupta2018}.
